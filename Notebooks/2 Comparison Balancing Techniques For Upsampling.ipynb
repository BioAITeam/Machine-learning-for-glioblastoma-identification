{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c00f6332",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b75ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Methods\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from yellowbrick.classifier import ClassificationReport \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "#Tools\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "import string \n",
    "import time as tm\n",
    "import os\n",
    "from scipy.sparse import csr_matrix \n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "\n",
    "#Class balance\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from imblearn.under_sampling import ClusterCentroids \n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae6147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "duration = 1000  # milliseconds\n",
    "freq = 440  # Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be25a55b",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ba576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_metrics(X_train,X_test,y_train,y_test,HO=True,CV=True):    \n",
    "    def metrics(model):\n",
    "        if HO == True:\n",
    "            print(\"\\nHold-Out in process...\")\n",
    "            start_time = tm.time()\n",
    "            model.fit(X_train, y_train) \n",
    "            TIME = tm.time() - start_time \n",
    "            print(\"Time, Training: {0:.4f} [seconds]\".format(TIME))\n",
    "            start_time = tm.time()\n",
    "            y_pred = model.predict(X_test)\n",
    "            TIME = tm.time() - start_time \n",
    "            print(\"Time, Prediction: {0:.4f} [seconds]\".format(TIME))\n",
    "\n",
    "            accuracy_s  = accuracy_score(y_test,y_pred) \n",
    "            print('accuracy_score: {0:.4f}'.format(accuracy_s))\n",
    "            f1_s        = f1_score(y_test,y_pred,average='weighted')\n",
    "            print('f1_score: {0:.4f}'.format(f1_s))\n",
    "            recall_s    = recall_score(y_test,y_pred,average='weighted')\n",
    "            print('recall_score: {0:.4f}'.format(recall_s))\n",
    "            precision_s = precision_score(y_test,y_pred,average='weighted')\n",
    "            print('precision_score: {0:.4f}'.format(precision_s))\n",
    "\n",
    "            if type(list(np.unique(np.array(y_train)))[0]).__name__ == 'str': #If the classes are categorical with string names\n",
    "                le           = LabelEncoder() \n",
    "                le.fit(list(np.unique(np.array(y_train)))) \n",
    "                y_test_coded = le.transform(y_test) \n",
    "                y_pred_coded = le.transform(y_pred) \n",
    "                mse_s        = MSE(y_test_coded,y_pred_coded)\n",
    "                print('MSE: {0:.4f}'.format(mse_s))\n",
    "            else:\n",
    "                mse_s        = MSE(y_test,y_pred)\n",
    "                print('MSE: {0:.4f}'.format(mse_s))\n",
    "\n",
    "            if len(list(np.unique(np.array(y_train)))) > 2: #For multiclass classification, more than 2 classes\n",
    "                y_pred_proba = model.predict_proba(X_test)[:]\n",
    "                roc_s        = roc_auc_score(y_test, y_pred_proba, multi_class='ovo', average='weighted')\n",
    "                print('ROC_AUC: {0:.4f}'.format(roc_s))            \n",
    "            else:\n",
    "                y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "                roc_s        = roc_auc_score(y_test, y_pred_proba, multi_class='ovo', average='weighted')\n",
    "                print('ROC_AUC: {0:.4f}'.format(roc_s))\n",
    "\n",
    "            ck_s         = cohen_kappa_score(y_test,y_pred)\n",
    "            print('CK: {0:.4f}'.format(ck_s))\n",
    "        \n",
    "        if CV == True:\n",
    "            print('\\nCross-Validation in process...')\n",
    "            start_time = tm.time() \n",
    "            kfold = model_selection.KFold(n_splits=10)\n",
    "            y_CV = np.concatenate((y_train,y_test))\n",
    "            if \"GaussianNB\" in str(name) or \"LinearDiscriminantAnalysis\" in str(name) or \"QuadraticDiscriminantAnalysis\" in str(name):\n",
    "                X_CV = np.concatenate((X_train,X_test))\n",
    "                cv_results = np.array(model_selection.cross_val_score(model, X_CV, y_CV, cv=kfold, scoring='accuracy', n_jobs=-3))\n",
    "            else:\n",
    "                X_CV = np.concatenate((X_train.toarray(),X_test.toarray()))\n",
    "                X_CV = csr_matrix(X_CV)\n",
    "                cv_results = np.array(model_selection.cross_val_score(model, X_CV, y_CV, cv=kfold, scoring='accuracy', n_jobs=-3))\n",
    "\n",
    "            cv_results = cv_results[np.logical_not(np.isnan(cv_results))] \n",
    "            TIME = tm.time() - start_time \n",
    "            print(\"Time, CV: {0:.4f} [seconds]\".format(TIME))\n",
    "            print('CV: {0:.4f} {1:.4f}'.format(cv_results.mean(),cv_results.std()))\n",
    "\n",
    "    for name in classifiers:\n",
    "        print (\"---------------------------------------------------------------------------------\\n\") \n",
    "        print(str(name))\n",
    "        if \"GaussianNB\" in str(name) or \"LinearDiscriminantAnalysis\" in str(name) or \"QuadraticDiscriminantAnalysis\" in str(name):\n",
    "            X_train=csr_matrix(X_train) \n",
    "            X_test =csr_matrix(X_test) \n",
    "            X_train=X_train.toarray() \n",
    "            X_test=X_test.toarray() \n",
    "        else:\n",
    "            X_train=csr_matrix(X_train)\n",
    "            X_test=csr_matrix(X_test)\n",
    "            \n",
    "        metrics(name)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_balance_over_sampling(features, labels, HO=False, CV=True, methods_list=[\"RandomOverSampler\"]):\n",
    "    \n",
    "    best_acc=list()\n",
    "    for method in methods_list:\n",
    "        if method == \"RandomOverSampler\":\n",
    "            print(method)\n",
    "            print(\"originals labels unique: \",np.unique(labels, return_counts=True)) \n",
    "            X_train, X_test, y_train, y_test = train_test_split(features, labels, \n",
    "                                                                test_size=0.20, random_state=21, stratify=labels)\n",
    "            sampler = RandomOverSampler(random_state=21) \n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)             \n",
    "            print(\"y_train labels unique:   \",np.unique(y_train, return_counts=True))\n",
    "            print(\"y_test labels unique:    \",np.unique(y_test, return_counts=True)) \n",
    "            classifier_metrics(X_train,X_test,y_train,y_test,HO=HO,CV=CV)\n",
    "            \n",
    "        elif method == \"SMOTE\":\n",
    "            print(method)\n",
    "            print(\"originals labels unique: \",np.unique(labels, return_counts=True)) \n",
    "            X_train, X_test, y_train, y_test = train_test_split(features, labels, \n",
    "                                                                test_size=0.20, random_state=21, stratify=labels)\n",
    "            sampler = SMOTE(random_state=21,n_jobs=-1) \n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)             \n",
    "            print(\"y_train labels unique:   \",np.unique(y_train, return_counts=True))\n",
    "            print(\"y_test labels unique:    \",np.unique(y_test, return_counts=True)) \n",
    "            classifier_metrics(X_train,X_test,y_train,y_test,HO=HO,CV=CV)\n",
    "\n",
    "        elif method == \"SMOTEN\":\n",
    "            print(method)\n",
    "            print(\"originals labels unique: \",np.unique(labels, return_counts=True)) \n",
    "            X_train, X_test, y_train, y_test = train_test_split(features, labels, \n",
    "                                                                test_size=0.20, random_state=21, stratify=labels)\n",
    "            sampler = SMOTEN(random_state=21,n_jobs=-1)\n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)             \n",
    "            print(\"y_train labels unique:   \",np.unique(y_train, return_counts=True))\n",
    "            print(\"y_test labels unique:    \",np.unique(y_test, return_counts=True)) \n",
    "            classifier_metrics(X_train,X_test,y_train,y_test,HO=HO,CV=CV)\n",
    "            \n",
    "        elif method == \"ADASYN\":\n",
    "            print(method)\n",
    "            print(\"originals labels unique: \",np.unique(labels, return_counts=True)) \n",
    "            X_train, X_test, y_train, y_test = train_test_split(features, labels, \n",
    "                                                                test_size=0.20, random_state=21, stratify=labels)\n",
    "            sampler = ADASYN(random_state=21,n_jobs=-1) \n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)             \n",
    "            print(\"y_train labels unique:   \",np.unique(y_train, return_counts=True))\n",
    "            print(\"y_test labels unique:    \",np.unique(y_test, return_counts=True)) \n",
    "            classifier_metrics(X_train,X_test,y_train,y_test,HO=HO,CV=CV)\n",
    "            \n",
    "        elif method == \"BorderlineSMOTE\":\n",
    "            print(method)\n",
    "            print(\"originals labels unique: \",np.unique(labels, return_counts=True)) \n",
    "            X_train, X_test, y_train, y_test = train_test_split(features, labels, \n",
    "                                                                test_size=0.20, random_state=21, stratify=labels)\n",
    "            sampler = BorderlineSMOTE(random_state=21,n_jobs=-1) \n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)             \n",
    "            print(\"y_train labels unique:   \",np.unique(y_train, return_counts=True))\n",
    "            print(\"y_test labels unique:    \",np.unique(y_test, return_counts=True)) \n",
    "            classifier_metrics(X_train,X_test,y_train,y_test,HO=HO,CV=CV)\n",
    "            \n",
    "        elif method == \"KMeansSMOTE\":\n",
    "            print(method)\n",
    "            print(\"originals labels unique: \",np.unique(labels, return_counts=True)) \n",
    "            X_train, X_test, y_train, y_test = train_test_split(features, labels, \n",
    "                                                                test_size=0.20, random_state=21, stratify=labels)\n",
    "            sampler = KMeansSMOTE(random_state=21,n_jobs=-1, k_neighbors=np.unique(y_test).shape[0]) \n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)             \n",
    "            print(\"y_train labels unique:   \",np.unique(y_train, return_counts=True))\n",
    "            print(\"y_test labels unique:    \",np.unique(y_test, return_counts=True)) \n",
    "            classifier_metrics(X_train,X_test,y_train,y_test,HO=HO,CV=CV)\n",
    "            \n",
    "        elif method == \"SVMSMOTE\":\n",
    "            print(method)\n",
    "            print(\"originals labels unique: \",np.unique(labels, return_counts=True)) \n",
    "            X_train, X_test, y_train, y_test = train_test_split(features, labels, \n",
    "                                                                test_size=0.20, random_state=21, stratify=labels)\n",
    "            sampler = SVMSMOTE(random_state=8,n_jobs=-1) \n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)             \n",
    "            print(\"y_train labels unique:   \",np.unique(y_train, return_counts=True))\n",
    "            print(\"y_test labels unique:    \",np.unique(y_test, return_counts=True)) \n",
    "            classifier_metrics(X_train,X_test,y_train,y_test,HO=HO,CV=CV)\n",
    "            \n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b34e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "def CM_viz(y_test, y_pred, classes, name,\n",
    "               path_img_base = './images',nrows=1,ncols=1,size_text_legend=25,size_text_title=25,title=\"\",\n",
    "           size_text_xy_labels=25,size_text_xy_tick=25,\n",
    "          size_num_inter=25):\n",
    "    if not os.path.exists(path_img_base):\n",
    "        os.makedirs(path_img_base)\n",
    "    \n",
    "    if ncols==nrows and ncols==1:\n",
    "        nrows=1\n",
    "        ncols=1\n",
    "        fig = plt.figure(figsize=(20*ncols,20*nrows))\n",
    "        conf = confusion_matrix(y_test, y_pred) \n",
    "        annot_kws={'fontsize':size_num_inter, 'verticalalignment':'center' } \n",
    "        ax = sns.heatmap(conf, annot=True, cmap='Blues',fmt = 'd',annot_kws= annot_kws, \n",
    "                                      xticklabels=np.unique(classes), yticklabels=np.unique(classes)) \n",
    "        cbar = ax.collections[0].colorbar # use matplotlib.colorbar.Colorbar object\n",
    "        cbar.ax.tick_params(labelsize=size_text_xy_tick) # here set the labelsize \n",
    "        ax.xaxis.set_tick_params(labelsize=size_text_xy_tick,rotation=90)\n",
    "        ax.yaxis.set_tick_params(labelsize=size_text_xy_tick,rotation=0)\n",
    "        ax.set_xlabel('Predicted Values',fontsize=size_text_xy_labels)\n",
    "        ax.set_ylabel('Actual Values',fontsize=size_text_xy_labels)\n",
    "        ax.set_title(title,fontsize=size_text_title)\n",
    "        ax.figure.subplots_adjust(right=0.8)\n",
    "        ax.figure.savefig(path_figures+\"/\"+name+\"_CM\"+\".pdf\", bbox_inches = \"tight\", format='pdf')\n",
    "    else:\n",
    "        conf = confusion_matrix(y_test, y_pred) \n",
    "        annot_kws={'fontsize':size_num_inter, 'verticalalignment':'center' }\n",
    "\n",
    "        ax = sns.heatmap(conf, annot=True, cmap='Blues',fmt = 'd',annot_kws= annot_kws, \n",
    "                                      xticklabels=np.unique(classes), yticklabels=np.unique(classes)) \n",
    "        cbar = ax.collections[0].colorbar # use matplotlib.colorbar.Colorbar object\n",
    "        cbar.ax.tick_params(labelsize=size_text_xy_tick) # here set the labelsize \n",
    "        ax.xaxis.set_tick_params(labelsize=size_text_xy_tick,rotation=90)\n",
    "        ax.yaxis.set_tick_params(labelsize=size_text_xy_tick,rotation=0)\n",
    "        ax.set_xlabel('Predicted Values',fontsize=size_text_xy_labels)\n",
    "        ax.set_ylabel('Actual Values',fontsize=size_text_xy_labels)\n",
    "        ax.set_title(title,fontsize=size_text_title)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad95c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_complete_s1(path):\n",
    "    df_complete=pd.read_csv(path)\n",
    "    \n",
    "    # Scenario 1: Tumor_Core & Tumor_Periphery\n",
    "    # Se procede a eliminar el N_Periphery\n",
    "\n",
    "    df2 = df_complete.copy()\n",
    "    df2.drop(df2[df2.classes == \"NP\"].index, inplace=True)  \n",
    "    \n",
    "    # Eliminamos los labels\n",
    "    features = df2.copy()\n",
    "    features = features.drop(['classes'], axis=1)\n",
    "    \n",
    "    #Extraemos los labels\n",
    "    labels = df2.copy()\n",
    "    labels = labels['classes'].values\n",
    "    \n",
    "    return features,labels\n",
    "\n",
    "def load_data_complete_s2(path):\n",
    "    df_complete=pd.read_csv(path)\n",
    "    \n",
    "    # Scenario 2: Normal_Periphery & Tumor_Periphery\n",
    "    # Se procede a eliminar el T_Core\n",
    "\n",
    "    df2 = df_complete.copy()\n",
    "    df2.drop(df2[df2.classes == \"TC\"].index, inplace=True)  \n",
    "    \n",
    "    # Eliminamos los labels\n",
    "    features = df2.copy()\n",
    "    features = features.drop(['classes'], axis=1)\n",
    "    \n",
    "    #Extraemos los labels\n",
    "    labels = df2.copy()\n",
    "    labels = labels['classes'].values\n",
    "    \n",
    "    return features,labels\n",
    "\n",
    "def load_data_complete_s3(path):\n",
    "    df_complete=pd.read_csv(path)\n",
    "    # Scenario 3: Tumor_Periphery&Core & Normal_Periphery\n",
    "    \n",
    "    # Se procede aislar al N_Periphery\n",
    "    df2 = df_complete.copy()\n",
    "    df2.drop(df2[df2.classes == \"TP\"].index, inplace=True)  \n",
    "    df2.drop(df2[df2.classes == \"TC\"].index, inplace=True)  \n",
    "    \n",
    "    # Eliminamos el N_Periphery\n",
    "    df3 = df_complete.copy()\n",
    "    df3.drop(df3[df3.classes == \"NP\"].index, inplace=True) \n",
    "    \n",
    "    # y luego se procede a renombrar la columna classes con T_PC, al quedar la unión de estas\n",
    "    df3[\"classes\"] = \"TPC\"\n",
    "    \n",
    "    # Se procede a crear el DF ya con las clases que corresponde al Escenario 3: Tumor_Periphery&Core & Normal_Periphery\n",
    "    #df2 N_Periphery\n",
    "    #df3 T_PC\n",
    "\n",
    "    df4 = pd.concat([df2,df3]).reset_index(drop=True) \n",
    "    \n",
    "    # Eliminamos los labels\n",
    "    features = df4.copy()\n",
    "    features = features.drop(['classes'], axis=1)\n",
    "    \n",
    "    #Extraemos los labels\n",
    "    labels = df4.copy()\n",
    "\n",
    "    labels = labels['classes'].values\n",
    "    \n",
    "    return features,labels\n",
    "\n",
    "def load_data_complete_s4(path):\n",
    "    df_complete=pd.read_csv(path)\n",
    "    \n",
    "    # Scenario 4 new: Tumor_Core & Tumor_Periphery & N_Periphery\n",
    "\n",
    "    # Eliminamos los labels\n",
    "    features = df_complete.copy()\n",
    "    features = features.drop(['classes'], axis=1)\n",
    "    \n",
    "    #Extraemos los labels\n",
    "    labels = df_complete.copy()\n",
    "    labels = labels['classes'].values\n",
    "    \n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb6e59",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54c579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Data/DATA_Complete_GBM.csv'\n",
    "\n",
    "featuress1,labelss1=load_data_complete_s1(path)\n",
    "featuress2,labelss2=load_data_complete_s2(path)\n",
    "featuress3,labelss3=load_data_complete_s3(path)\n",
    "featuress4,labelss4=load_data_complete_s4(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65853643",
   "metadata": {},
   "source": [
    "# Machine learning application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb7b76c",
   "metadata": {},
   "source": [
    "## Scenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44689cda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Without balancing\n",
    "X_train, X_test, y_train, y_test = train_test_split(featuress1, labelss1, test_size=0.20, random_state=21, stratify=labelss1)\n",
    "\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape) \n",
    "print(np.unique(y_train, return_counts=True))\n",
    "print(np.unique(y_test, return_counts=True)) \n",
    "\n",
    "#ML Models\n",
    "classifiers=[\n",
    "    XGBClassifier(eval_metric='mlogloss',n_jobs=-1),\n",
    "    LogisticRegression(solver='liblinear',n_jobs=-1),\n",
    "    SVC(probability=True,kernel='linear'),\n",
    "    GradientBoostingClassifier()  \n",
    "    ] \n",
    "\n",
    "#Deploy aggregate metrics \n",
    "classifier_metrics(X_train,X_test,y_train,y_test,HO=True,CV=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML Models\n",
    "classifiers=[\n",
    "    XGBClassifier(eval_metric='mlogloss',n_jobs=-1),\n",
    "    LogisticRegression(solver='liblinear',n_jobs=-1),\n",
    "    SVC(probability=True,kernel='linear'),\n",
    "    GradientBoostingClassifier()    \n",
    "    ] \n",
    "\n",
    "'''\n",
    "methods_list=[\"RandomOverSampler\",  \n",
    "              \"SMOTE\",\n",
    "              \"SMOTEN\",\n",
    "              \"ADASYN\",\n",
    "              \"BorderlineSMOTE\",\n",
    "              #\"KMeansSMOTE\", #No in scenario 1,2, and 4, only use if you have a lot of data\n",
    "              \"SVMSMOTE\"\n",
    "             ]\n",
    "'''\n",
    "\n",
    "class_balance_over_sampling(featuress1, labelss1, HO=True, CV=False, \n",
    "                            methods_list=[\"ADASYN\",\"SVMSMOTE\",\"SMOTE\",\"RandomOverSampler\",\n",
    "                                          \"SMOTEN\",\"BorderlineSMOTE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a0e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20d6d2",
   "metadata": {},
   "source": [
    "## Scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b37b7f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Without balancing\n",
    "X_train, X_test, y_train, y_test = train_test_split(featuress2, labelss2, test_size=0.20, random_state=21, stratify=labelss2)\n",
    "\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape) \n",
    "print(np.unique(y_train, return_counts=True))\n",
    "print(np.unique(y_test, return_counts=True)) \n",
    "\n",
    "#ML Models\n",
    "classifiers=[\n",
    "    LogisticRegression(solver='liblinear',n_jobs=-1),\n",
    "    SVC(probability=True,kernel='linear'),\n",
    "    RandomForestClassifier(n_jobs=-1,random_state=8),\n",
    "    ExtraTreesClassifier()    \n",
    "    ] \n",
    "\n",
    "#Deploy aggregate metrics \n",
    "classifier_metrics(X_train,X_test,y_train,y_test,HO=True,CV=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c3e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ML Models\n",
    "classifiers=[\n",
    "    LogisticRegression(solver='liblinear',n_jobs=-1),\n",
    "    SVC(probability=True,kernel='linear'),\n",
    "    RandomForestClassifier(n_jobs=-1,random_state=8),\n",
    "    ExtraTreesClassifier()   \n",
    "    ] \n",
    "\n",
    "'''\n",
    "methods_list=[\"RandomOverSampler\",  \n",
    "              \"SMOTE\",\n",
    "              \"SMOTEN\",\n",
    "              \"ADASYN\",\n",
    "              \"BorderlineSMOTE\",\n",
    "              #\"KMeansSMOTE\", #No in scenario 1,2, and 4, only use if you have a lot of data\n",
    "              \"SVMSMOTE\"\n",
    "             ]\n",
    "'''\n",
    "\n",
    "class_balance_over_sampling(featuress2, labelss2, HO=True, CV=False, \n",
    "                            methods_list=[\"ADASYN\",\"SVMSMOTE\",\"SMOTE\",\"RandomOverSampler\",\n",
    "                                          \"SMOTEN\",\"BorderlineSMOTE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8df4091",
   "metadata": {},
   "outputs": [],
   "source": [
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9078c73d",
   "metadata": {},
   "source": [
    "## Scenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caaea0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Without balancing\n",
    "X_train, X_test, y_train, y_test = train_test_split(featuress3, labelss3, test_size=0.20, random_state=21, stratify=labelss3)\n",
    "\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape) \n",
    "print(np.unique(y_train, return_counts=True))\n",
    "print(np.unique(y_test, return_counts=True)) \n",
    "\n",
    "#ML Models\n",
    "classifiers=[\n",
    "    GradientBoostingClassifier(random_state=8),\n",
    "    XGBClassifier(eval_metric='mlogloss',n_jobs=-1),\n",
    "    RandomForestClassifier(n_jobs=-1,random_state=32),\n",
    "    SVC(probability=True,kernel='linear')\n",
    "    ] \n",
    "\n",
    "#Deploy aggregate metrics \n",
    "classifier_metrics(X_train,X_test,y_train,y_test,HO=True,CV=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bca5111",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ML Models\n",
    "classifiers=[\n",
    "    GradientBoostingClassifier(random_state=8),\n",
    "    XGBClassifier(eval_metric='mlogloss',n_jobs=-1),\n",
    "    RandomForestClassifier(n_jobs=-1,random_state=32),\n",
    "    SVC(probability=True,kernel='linear')\n",
    "    ] \n",
    "\n",
    "'''\n",
    "methods_list=[\"RandomOverSampler\",  \n",
    "              \"SMOTE\",\n",
    "              \"SMOTEN\",\n",
    "              \"ADASYN\",\n",
    "              \"BorderlineSMOTE\",\n",
    "              #\"KMeansSMOTE\", #No in scenario 1,2, and 4, only use if you have a lot of data\n",
    "              \"SVMSMOTE\"\n",
    "             ]\n",
    "'''\n",
    "\n",
    "class_balance_over_sampling(featuress3, labelss3, HO=True, CV=False, \n",
    "                            methods_list=[\"ADASYN\",\"SVMSMOTE\",\"SMOTE\",\"RandomOverSampler\",\n",
    "                                          \"SMOTEN\",\"BorderlineSMOTE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5ab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f434e",
   "metadata": {},
   "source": [
    "## Scenario 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b747786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without balancing\n",
    "X_train, X_test, y_train, y_test = train_test_split(featuress4, labelss4, test_size=0.20, random_state=21, stratify=labelss4)\n",
    "\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape) \n",
    "print(np.unique(y_train, return_counts=True))\n",
    "print(np.unique(y_test, return_counts=True)) \n",
    "\n",
    "#ML Models\n",
    "classifiers=[\n",
    "    XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=4,\n",
    "                  min_child_weight=4, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                  objective= 'binary:logistic', nthread=4, seed=27, eval_metric='mlogloss', n_jobs=-1),\n",
    "    GradientBoostingClassifier(),\n",
    "    LogisticRegression(solver='liblinear',n_jobs=-1),\n",
    "    SVC(probability=True,kernel='linear')    \n",
    "    ] \n",
    "\n",
    "#Deploy aggregate metrics \n",
    "classifier_metrics(X_train,X_test,y_train,y_test,HO=True,CV=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacbb33a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ML Models\n",
    "classifiers=[\n",
    "    XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=4,\n",
    "                  min_child_weight=4, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                  objective= 'binary:logistic', nthread=4, seed=27, eval_metric='mlogloss', n_jobs=-1),\n",
    "    GradientBoostingClassifier(),\n",
    "    LogisticRegression(solver='liblinear',n_jobs=-1),\n",
    "    SVC(probability=True,kernel='linear')  \n",
    "    ] \n",
    "\n",
    "'''\n",
    "methods_list=[\"RandomOverSampler\",  \n",
    "              \"SMOTE\",\n",
    "              \"SMOTEN\",\n",
    "              \"ADASYN\",\n",
    "              \"BorderlineSMOTE\",\n",
    "              #\"KMeansSMOTE\", #No in scenario 1,2, and 4, only use if you have a lot of data\n",
    "              \"SVMSMOTE\"\n",
    "             ]\n",
    "'''\n",
    "\n",
    "class_balance_over_sampling(featuress4, labelss4, HO=True, CV=False, \n",
    "                            methods_list=[\"ADASYN\",\"SVMSMOTE\",\"SMOTE\",\"RandomOverSampler\",\n",
    "                                          \"SMOTEN\",\"BorderlineSMOTE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3097abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ad030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
