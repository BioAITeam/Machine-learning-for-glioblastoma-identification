{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c00f6332",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b75ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Methods\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from yellowbrick.classifier import ClassificationReport \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "#Tools\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "import string \n",
    "import time as tm\n",
    "import os\n",
    "from scipy.sparse import csr_matrix \n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "\n",
    "#Class balance\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from imblearn.under_sampling import ClusterCentroids \n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae6147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "duration = 1000  # milliseconds\n",
    "freq = 440  # Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb6e59",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384104ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_complete_s1(path):\n",
    "    df_complete=pd.read_csv(path)\n",
    "    \n",
    "    # Scenario 1: Tumor_Core & Tumor_Periphery\n",
    "    # Se procede a eliminar el N_Periphery\n",
    "\n",
    "    df2 = df_complete.copy()\n",
    "    df2.drop(df2[df2.classes == \"NP\"].index, inplace=True)  \n",
    "    \n",
    "    # Eliminamos los labels\n",
    "    features = df2.copy()\n",
    "    features = features.drop(['classes'], axis=1)\n",
    "    \n",
    "    #Extraemos los labels\n",
    "    labels = df2.copy()\n",
    "    labels = labels['classes'].values\n",
    "    \n",
    "    return features,labels\n",
    "\n",
    "def load_data_complete_s2(path):\n",
    "    df_complete=pd.read_csv(path)\n",
    "    \n",
    "    # Scenario 2: Normal_Periphery & Tumor_Periphery\n",
    "    # Se procede a eliminar el T_Core\n",
    "\n",
    "    df2 = df_complete.copy()\n",
    "    df2.drop(df2[df2.classes == \"TC\"].index, inplace=True)  \n",
    "    \n",
    "    # Eliminamos los labels\n",
    "    features = df2.copy()\n",
    "    features = features.drop(['classes'], axis=1)\n",
    "    \n",
    "    #Extraemos los labels\n",
    "    labels = df2.copy()\n",
    "    labels = labels['classes'].values\n",
    "    \n",
    "    return features,labels\n",
    "\n",
    "def load_data_complete_s3(path):\n",
    "    df_complete=pd.read_csv(path)\n",
    "    # Scenario 3: Tumor_Periphery&Core & Normal_Periphery\n",
    "    \n",
    "    # Se procede aislar al N_Periphery\n",
    "    df2 = df_complete.copy()\n",
    "    df2.drop(df2[df2.classes == \"TP\"].index, inplace=True)  \n",
    "    df2.drop(df2[df2.classes == \"TC\"].index, inplace=True)  \n",
    "    \n",
    "    # Eliminamos el N_Periphery\n",
    "    df3 = df_complete.copy()\n",
    "    df3.drop(df3[df3.classes == \"NP\"].index, inplace=True) \n",
    "    \n",
    "    # y luego se procede a renombrar la columna classes con T_PC, al quedar la uni√≥n de estas\n",
    "    df3[\"classes\"] = \"TPC\"\n",
    "    \n",
    "    # Se procede a crear el DF ya con las clases que corresponde al Escenario 3: Tumor_Periphery&Core & Normal_Periphery\n",
    "    #df2 N_Periphery\n",
    "    #df3 T_PC\n",
    "\n",
    "    df4 = pd.concat([df2,df3]).reset_index(drop=True) \n",
    "    \n",
    "    # Eliminamos los labels\n",
    "    features = df4.copy()\n",
    "    features = features.drop(['classes'], axis=1)\n",
    "    \n",
    "    #Extraemos los labels\n",
    "    labels = df4.copy()\n",
    "\n",
    "    labels = labels['classes'].values\n",
    "    \n",
    "    return features,labels\n",
    "\n",
    "def load_data_complete_s4(path):\n",
    "    df_complete=pd.read_csv(path)\n",
    "    \n",
    "    # Scenario 4 new: Tumor_Core & Tumor_Periphery & N_Periphery\n",
    "\n",
    "    # Eliminamos los labels\n",
    "    features = df_complete.copy()\n",
    "    features = features.drop(['classes'], axis=1)\n",
    "    \n",
    "    #Extraemos los labels\n",
    "    labels = df_complete.copy()\n",
    "    labels = labels['classes'].values\n",
    "    \n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d69c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "def CM_viz(y_test, y_pred, classes, name,\n",
    "               path_img_base = './images',nrows=1,ncols=1,size_text_legend=25,size_text_title=25,title=\"\",\n",
    "           size_text_xy_labels=25,size_text_xy_tick=25,\n",
    "          size_num_inter=25):\n",
    "    if not os.path.exists(path_img_base):\n",
    "        os.makedirs(path_img_base)\n",
    "    \n",
    "    if ncols==nrows and ncols==1:\n",
    "        nrows=1\n",
    "        ncols=1\n",
    "        fig = plt.figure(figsize=(20*ncols,20*nrows))\n",
    "        conf = confusion_matrix(y_test, y_pred) \n",
    "        annot_kws={'fontsize':size_num_inter, 'verticalalignment':'center' } \n",
    "        ax = sns.heatmap(conf, annot=True, cmap='Blues',fmt = 'd',annot_kws= annot_kws, \n",
    "                                      xticklabels=np.unique(classes), yticklabels=np.unique(classes)) \n",
    "        cbar = ax.collections[0].colorbar # use matplotlib.colorbar.Colorbar object\n",
    "        cbar.ax.tick_params(labelsize=size_text_xy_tick) # here set the labelsize \n",
    "        ax.xaxis.set_tick_params(labelsize=size_text_xy_tick,rotation=90)\n",
    "        ax.yaxis.set_tick_params(labelsize=size_text_xy_tick,rotation=0)\n",
    "        ax.set_xlabel('Predicted Values',fontsize=size_text_xy_labels)\n",
    "        ax.set_ylabel('Actual Values',fontsize=size_text_xy_labels)\n",
    "        ax.set_title(title,fontsize=size_text_title)\n",
    "        ax.figure.subplots_adjust(right=0.8)\n",
    "        ax.figure.savefig(path_img_base+\"/\"+name+\"_CM\"+\".pdf\", bbox_inches = \"tight\", format='pdf')\n",
    "    else:\n",
    "        conf = confusion_matrix(y_test, y_pred) \n",
    "        annot_kws={'fontsize':size_num_inter, 'verticalalignment':'center' }\n",
    "\n",
    "        ax = sns.heatmap(conf, annot=True, cmap='Blues',fmt = 'd',annot_kws= annot_kws, \n",
    "                                      xticklabels=np.unique(classes), yticklabels=np.unique(classes)) \n",
    "        cbar = ax.collections[0].colorbar # use matplotlib.colorbar.Colorbar object\n",
    "        cbar.ax.tick_params(labelsize=size_text_xy_tick) # here set the labelsize \n",
    "        ax.xaxis.set_tick_params(labelsize=size_text_xy_tick,rotation=90)\n",
    "        ax.yaxis.set_tick_params(labelsize=size_text_xy_tick,rotation=0)\n",
    "        ax.set_xlabel('Predicted Values',fontsize=size_text_xy_labels)\n",
    "        ax.set_ylabel('Actual Values',fontsize=size_text_xy_labels)\n",
    "        ax.set_title(title,fontsize=size_text_title)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f35a1db",
   "metadata": {},
   "source": [
    "# LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54c579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Data/DATA_Complete_GBM.csv'\n",
    "\n",
    "featuress1,labelss1=load_data_complete_s1(path)\n",
    "featuress2,labelss2=load_data_complete_s2(path)\n",
    "featuress3,labelss3=load_data_complete_s3(path)\n",
    "featuress4,labelss4=load_data_complete_s4(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65853643",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING APPLICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb7b76c",
   "metadata": {},
   "source": [
    "## Scenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ef1313",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"originals labels unique: \",np.unique(labelss1, return_counts=True)) \n",
    "X_trains1, X_tests1, y_trains1, y_tests1 = train_test_split(featuress1, labelss1, \n",
    "                                                    test_size=0.20, random_state=21, stratify=labelss1)\n",
    "sampler = ADASYN(random_state=21,n_jobs=-1) \n",
    "X_trains1, y_trains1 = sampler.fit_resample(X_trains1, y_trains1)             \n",
    "print(\"y_train labels unique:   \",np.unique(y_trains1, return_counts=True))\n",
    "print(\"y_test labels unique:    \",np.unique(y_tests1, return_counts=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db8f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trains1=csr_matrix(X_trains1)\n",
    "X_tests1=csr_matrix(X_tests1)\n",
    "models1 = XGBClassifier(eval_metric='mlogloss',n_jobs=-1)\n",
    "models1.fit(X_trains1, y_trains1) \n",
    "y_preds1 = models1.predict(X_tests1)\n",
    "accuracy_s1  = accuracy_score(y_tests1,y_preds1) \n",
    "print('accuracy_score: {0:.4f}'.format(accuracy_s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eff179",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=list(featuress1.columns)\n",
    "viz = FeatureImportances(models1,labels=feature_names,topn=20)\n",
    "viz.fit(X_trains1, y_trains1)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e06ca9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importances=pd.DataFrame({'features':featuress1.columns,'feature_importance':models1.feature_importances_})\n",
    "feature_importances.sort_values('feature_importance',ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d816cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fi=list(feature_importances.sort_values('feature_importance',ascending=False)[:20]['features'])\n",
    "for i in range(len(fi)):\n",
    "    print(fi[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20c3534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features=featuress1.loc[:,fi]\n",
    "#features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20d6d2",
   "metadata": {},
   "source": [
    "## Scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb5329",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"originals labels unique: \",np.unique(labelss2, return_counts=True)) \n",
    "X_trains2, X_tests2, y_trains2, y_tests2 = train_test_split(featuress2, labelss2, \n",
    "                                                    test_size=0.20, random_state=21, stratify=labelss2)\n",
    "sampler = SVMSMOTE(random_state=8,n_jobs=-1) \n",
    "X_trains2, y_trains2 = sampler.fit_resample(X_trains2, y_trains2)             \n",
    "print(\"y_train labels unique:   \",np.unique(y_trains2, return_counts=True))\n",
    "print(\"y_test labels unique:    \",np.unique(y_tests2, return_counts=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a7cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models2 = LogisticRegression(solver='liblinear',n_jobs=-1)\n",
    "models2.fit(X_trains2, y_trains2) \n",
    "y_preds2 = models2.predict(X_tests2)\n",
    "accuracy_s2  = accuracy_score(y_tests2,y_preds2) \n",
    "print('accuracy_score: {0:.4f}'.format(accuracy_s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = FeatureImportances(models2, topn=20)\n",
    "viz.fit(X_trains2, y_trains2)\n",
    "viz.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2876e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://predictivehacks.com/feature-importance-in-python/\n",
    "#Las de una de las clases\n",
    "feature_importance=pd.DataFrame({'feature':list(featuress2.columns),'feature_importance':[i for i in models2.coef_[0]]})\n",
    "feature_importance.sort_values('feature_importance',ascending=False)[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7fde9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Las de otra de las clases\n",
    "cond1=(feature_importance[\"feature_importance\"]<0)\n",
    "feature_importance[cond1].sort_values('feature_importance',ascending=True)[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc230a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dado que se usa model.coef_ se recomienda usar esta forma con el valor absoluto\n",
    "#Porque los positivos son para una clase, y los negativos para la otra\n",
    "feature_importance=pd.DataFrame({'feature':list(featuress2.columns),'feature_importance':[abs(i) for i in models2.coef_[0]]})\n",
    "feature_importance.sort_values('feature_importance',ascending=False)[:20] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7689ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fi=list(feature_importance.sort_values('feature_importance',ascending=False)[:20]['feature'])\n",
    "for i in range(len(fi)):\n",
    "    print(fi[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features=featuress2.loc[:,fi]\n",
    "#features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9078c73d",
   "metadata": {},
   "source": [
    "## Scenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd255af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"originals labels unique: \",np.unique(labelss3, return_counts=True)) \n",
    "X_trains3, X_tests3, y_trains3, y_tests3 = train_test_split(featuress3, labelss3, \n",
    "                                                    test_size=0.20, random_state=21, stratify=labelss3)\n",
    "sampler = SMOTE(random_state=21,n_jobs=-1) \n",
    "X_trains3, y_trains3 = sampler.fit_resample(X_trains3, y_trains3)             \n",
    "print(\"y_train labels unique:   \",np.unique(y_trains3, return_counts=True))\n",
    "print(\"y_test labels unique:    \",np.unique(y_tests3, return_counts=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c73e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "models3 = GradientBoostingClassifier(random_state=8)\n",
    "models3.fit(X_trains3, y_trains3) \n",
    "y_preds3 = models3.predict(X_tests3)\n",
    "accuracy_s3  = accuracy_score(y_tests3,y_preds3) \n",
    "print('accuracy_score: {0:.4f}'.format(accuracy_s3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = FeatureImportances(models3, topn=20)\n",
    "viz.fit(X_trains3, y_trains3)\n",
    "viz.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41312caa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importances=pd.DataFrame({'features':featuress3.columns,'feature_importance':models3.feature_importances_})\n",
    "feature_importances.sort_values('feature_importance',ascending=False)[:20] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989233b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fi=list(feature_importances.sort_values('feature_importance',ascending=False)[:20]['features'])\n",
    "for i in range(len(fi)):\n",
    "    print(fi[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3d40bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features=featuress3.loc[:,fi]\n",
    "#features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f434e",
   "metadata": {},
   "source": [
    "## Scenario 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df347e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"originals labels unique: \",np.unique(labelss4, return_counts=True)) \n",
    "X_trains4, X_tests4, y_trains4, y_tests4 = train_test_split(featuress4, labelss4, \n",
    "                                                    test_size=0.20, random_state=21, stratify=labelss4)\n",
    "sampler = RandomOverSampler(random_state=21) \n",
    "X_trains4, y_trains4 = sampler.fit_resample(X_trains4, y_trains4)             \n",
    "print(\"y_train labels unique:   \",np.unique(y_trains4, return_counts=True))\n",
    "print(\"y_test labels unique:    \",np.unique(y_tests4, return_counts=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31517ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models4 = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=4,\n",
    "                  min_child_weight=4, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                  objective= 'binary:logistic', nthread=4, seed=27, eval_metric='mlogloss', n_jobs=-1)\n",
    "models4.fit(X_trains4, y_trains4) \n",
    "y_preds4 = models4.predict(X_tests4)\n",
    "accuracy_s4  = accuracy_score(y_tests4,y_preds4) \n",
    "print('accuracy_score: {0:.4f}'.format(accuracy_s4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec9630",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = FeatureImportances(models4, topn=20)\n",
    "viz.fit(X_trains4, y_trains4)\n",
    "viz.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70860434",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances=pd.DataFrame({'features':featuress4.columns,'feature_importance':models4.feature_importances_})\n",
    "feature_importances.sort_values('feature_importance',ascending=False)[:20] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi=list(feature_importances.sort_values('feature_importance',ascending=False)[:20]['features'])\n",
    "for i in range(len(fi)):\n",
    "    print(fi[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f98d601",
   "metadata": {},
   "source": [
    "# CONFUSION MATRICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142b40bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_text_legend=60\n",
    "size_text_title=95\n",
    "size_text_xy_labels=60\n",
    "size_text_xy_tick=60\n",
    "size_num_inter=95\n",
    "\n",
    "nrows=1\n",
    "ncols=4\n",
    "\n",
    "fig = plt.figure(figsize=(20*ncols,15*nrows))\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "i=1\n",
    "ax = fig.add_subplot(nrows, ncols, i)\n",
    "###############################################SCENARIO 1\n",
    "\n",
    "model_name = \"Scenario 1\"\n",
    "model_selected = models1\n",
    "y_preds1 = model_selected.predict(X_tests1)\n",
    "acc_score=accuracy_score(y_tests1,y_preds1) \n",
    "print('accuracy_score: {0:.4f}'.format(acc_score))\n",
    "\n",
    "y_pred_proba = model_selected.predict_proba(X_tests1)\n",
    "classes = np.unique(y_tests1)\n",
    "\n",
    "#title=\"Confusion Matrix for {}\".format(model_name)\n",
    "title=\"A\"\n",
    "CM_viz(y_tests1, y_preds1, classes, name=model_name, \n",
    "                            path_img_base = './images',nrows=nrows,ncols=ncols, \n",
    "                            size_text_legend=size_text_legend,size_text_title=size_text_title,title=title,\n",
    "       size_text_xy_labels=size_text_xy_labels,size_text_xy_tick=size_text_xy_tick,size_num_inter=size_num_inter)\n",
    "###############################################\n",
    "\n",
    "i=2\n",
    "ax = fig.add_subplot(nrows, ncols, i)\n",
    "###############################################SCENARIO 2\n",
    "\n",
    "model_name = \"Scenario 2\"\n",
    "model_selected = models2\n",
    "y_preds2 = model_selected.predict(X_tests2)\n",
    "acc_score=accuracy_score(y_tests2,y_preds2) \n",
    "print('accuracy_score: {0:.4f}'.format(acc_score))\n",
    "\n",
    "y_pred_proba = model_selected.predict_proba(X_tests2)\n",
    "classes = np.unique(y_tests2)\n",
    "\n",
    "#title=\"Confusion Matrix for {}\".format(model_name)\n",
    "title=\"B\"\n",
    "CM_viz(y_tests2, y_preds2, classes, name=model_name, \n",
    "                            path_img_base = './images',nrows=nrows,ncols=ncols, \n",
    "                            size_text_legend=size_text_legend,size_text_title=size_text_title,title=title,\n",
    "       size_text_xy_labels=size_text_xy_labels,size_text_xy_tick=size_text_xy_tick,size_num_inter=size_num_inter)\n",
    "###############################################\n",
    "\n",
    "i=3\n",
    "ax = fig.add_subplot(nrows, ncols, i)\n",
    "###############################################SCENARIO 3\n",
    "\n",
    "model_name = \"Scenario 3\"\n",
    "model_selected = models3\n",
    "y_preds3 = model_selected.predict(X_tests3)\n",
    "acc_score=accuracy_score(y_tests3,y_preds3) \n",
    "print('accuracy_score: {0:.4f}'.format(acc_score))\n",
    "\n",
    "y_pred_proba = model_selected.predict_proba(X_tests3)\n",
    "classes = nclasses = np.unique(y_tests3)\n",
    "\n",
    "#title=\"Confusion Matrix for {}\".format(model_name)\n",
    "title=\"C\"\n",
    "CM_viz(y_tests3, y_preds3, classes, name=model_name, \n",
    "                            path_img_base = './images',nrows=nrows,ncols=ncols, \n",
    "                            size_text_legend=size_text_legend,size_text_title=size_text_title,title=title,\n",
    "       size_text_xy_labels=size_text_xy_labels,size_text_xy_tick=size_text_xy_tick,size_num_inter=size_num_inter)\n",
    "###############################################\n",
    "\n",
    "i=4\n",
    "ax = fig.add_subplot(nrows, ncols, i)\n",
    "###############################################SCENARIO 4\n",
    "\n",
    "model_name = \"Scenario 4\"\n",
    "model_selected = models4\n",
    "y_preds4 = model_selected.predict(X_tests4)\n",
    "acc_score=accuracy_score(y_tests4,y_preds4) \n",
    "print('accuracy_score: {0:.4f}'.format(acc_score))\n",
    "\n",
    "y_pred_proba = model_selected.predict_proba(X_tests4)\n",
    "classes = np.unique(y_tests4)\n",
    "\n",
    "#title=\"Confusion Matrix for {}\".format(model_name)\n",
    "title=\"D\"\n",
    "CM_viz(y_tests4, y_preds4, classes, name=model_name, \n",
    "                            path_img_base = './images',nrows=nrows,ncols=ncols, \n",
    "                            size_text_legend=size_text_legend,size_text_title=size_text_title,title=title,\n",
    "       size_text_xy_labels=size_text_xy_labels,size_text_xy_tick=size_text_xy_tick,size_num_inter=size_num_inter)\n",
    "###############################################\n",
    "\n",
    "model_name = \"CM_GBM\"\n",
    "fig.savefig(\"./images\"+\"/\"+model_name+\"_CM\"+\".pdf\", bbox_inches = \"tight\", format='pdf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8340bc72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
